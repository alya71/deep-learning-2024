<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>The Platonic Representation Hypothesis</title>
      <meta property="og:title" content="The Platonic Representation Hypothesis" />
			<meta charset="UTF-8">
	<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">A Grand Unified Theory of Deep Learning</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="your_website">Maggie Shi</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="your_partner's_website">Alina Yang</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#does_x_do_y">Does X do Y?</a><br><br>
              <a href="#implications_and_limitations">Implications and limitations</a><br><br>
          </div>    m m m m m 
					
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/your_image_here.png" width=512px/>
		    </div>
		    <div class="margin-right-block">
						Caption for the image.
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
			    
            <p>The Transformer model was introduced in 2017 <a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.
Since then, it has achieved remarkable progress in a variety of tasks,
including computer vision <a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a> and natural language
processing <a href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>.</p>
<p>Due to how large many transformer models are, it is not trivial to
understand their behavior and determine how they could be modified. But,
such work would be greatly beneficial to determine how to reduce the
memory or computation needed for training, as well as increase
interpretability.</p>
<p>One avenue along which this has been pursued is by examining the
representations that different models use. Huh et al. propose a
platonic/ideal representation<a href="#fn4" class="footnote-ref"
id="fnref4" role="doc-noteref"><sup>4</sup></a>—the notion that there’s
a certain representation that is most effective at capturing data, and
many models get close to this. Such a result led us to hypothesize that
as the layer of a model increases, because it is approaching this ideal
representation, there is less variation in the weights of the attention
matrices. In other words, in the later layers we are more likely to be
making smaller adjustments to the hidden state of a given embedding.</p>
<p>Another reason we were motivated in believing that there can be
similarity between later adjacent layers is because of the success of
Recurrent Neural Networks<a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>. Before the transformer architecture
was introduced, tasks like translation (i.e. Google Translate) would use
Long Short Term Memory. The recurrent inductive bias that RNNs have has
been shown to be important on various natural language processing
tasks<a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a>, and thus, we hypothesized that
models might discover this after training.</p>
<p>As we were examining how the query, key, and value matrices varied
across layers of the model, we discovered that there is a strong,
statistically significant correlation between the key weight and query
weight matrices in the following four metrics: sparsity, rank,
froebenius norm, and condition number. This led us to experiment on two
novel methods of weight sharing between query and key. The first method
is sharing the exact weight matrix between query and key, relating the
two with a simple identity matrix. The second method is relating \(W_K\) by a Hadamard
product with \(W_Q\) and \(W_K\), which
preserves sparsity and ensures rank is non-increasing going from \(W_Q\) to \(W_K\). In both
these methods, less parameters are learned, which saves memory and
compute time, without sacrificing performance.</p>
			    
		    </div>
		    <div class="margin-right-block">
						Margin note that clarifies some detail #main-content-block for intro section.
		    </div>
		</div>

		<div class="content-margin-container" id="does_x_do_y">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Similarity across layers</h1>
				  In order to examine our hypothesis that later layers are more similar to one another, we examined the progression of various metrics of the query, key, and value matrices of each head across the layers. 

Our first metric was looking at the kernel alignment metric, which measures how similar two mappings are. Given two matrices $P, Q$, the kernel alignment metric is given by
\[ \frac{\text{Tr}(P Q)}{\sqrt{\text{Tr}(P^2) \cdot \text{Tr}(Q^2)}}. \]
Our inference was preformed on the large, uncased BERT model. For a layer index \(i\) ranging from 0 to 22 (inclusive), we computed the kernel alignment metric between the \(\{\text{query}, \text{key}, \text{value}\}\) weight matrix in head \(j\) in layer \(i\), and the corresponding matrix in head \(j\) in layer \(i+1\). Our results are displayed in the figures below \textcolor{red}{(steal from ``Similarity across layers within the same head") part of the paper}

As shown in the figures, the magnitude of the kernel alignment score remains relatively low for the query, key, and value matrices as we increase the layer. This does not support our hypothesis that later layers are more similar. \textcolor{red}{idk if we want to do analysis here?} Upon further reflection, such a result seems sensible—effectively, if two layers were relatively similar, applying them sequentially would not be all that different from just applying a scaled version of one of them. Therefore, if many of the layers were similar, such a network could be less adept at learning complex representations, so it makes sense for networks to fully utilize their parameters by having more distinct layers.

In addition to examining the kernel alignment metric, we also looked at sparsity, rank, the Frobenius norm, and the condition number. However, the most interesting trends observed with these metrics have to deal with the relationship between query and key matrices rather than their evolution through the layers, so the graphs are attached in the next section.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Another section</h1>
            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		    <div class="margin-right-block">
					A caption for the video could go here.
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Our four metrics</h1>
						Motivated by our observations when studying similarity across layers, we investigated the relationship between query and key matrices in the same head. 

<h2>Sparsity</h2>

One property of matrices is sparsity. In the purest sense, the sparseness of a matrix is measured by the number of entries it has that are equal to zero. Because our neural network weights are tuned with an optimizer, none of the weights are exactly equal to zero. So, to capture this notion of weights that are insignificant and which could be eliminated, we counted the number of weights in each matrix who's magnitude was less than $10^{-4}$. \textcolor{red}{idk if this is needed but—}this threshold was chosen somewhat arbitrarily, but the phenomena we observe occurs for other reasonable choices as well. 


\textcolor{red}{include graphics—maybe both bert and baby bert}

Just visually, it appears that there is a relationship between the number of near zero entries of the query and key weights, but not with the value weights (\textcolor{red}{base bert: least sparse matrices are layer 2, head 0 / 9 for both key and query. big bert: idk rn lol}). To verify this, we computed the correlation matrix between these values. The correlation between the number of near zero entries of the value matrix and either of the query or key was insignificant, but for both sizes of BERT, the correlation coefficient between the number of near zero entries of the query and key matrix was above $0.9$.

<figure>
        <p style="text-align: center;">
            $$
            \begin{array}{c|ccc}
                & \text{Q} & \text{K} & \text{V} \\
                \hline
                \text{Q} & 1 & 0.919 & 0.001 \\
                \text{K} & 0.919 & 1 & 0.108 \\
                \text{V} & 0.001 & 0.108 & 1
            \end{array}
            \qquad
            \begin{array}{c|ccc}
                & \text{Q} & \text{K} & \text{V} \\
                \hline
                \text{Q} & 1 & 0.904 & -0.167 \\
                \text{K} & 0.904 & 1 & -0.133 \\
                \text{V} & -0.167 & -0.133 & 1
            \end{array}
            $$
        </p>
        <figcaption style="text-align: center;">
            Correlation matrices for BERT large (left) and BERT base (right) for the number of entries close to zero.
        </figcaption>
    </figure>

<h2>Effective Rank</h2>
Another property of matrices is their rank, or the dimension of the span of its column vectors. Again, because of our optimization process, all of our matrices will almost certainly be full rank. We get around this by computing ``effective rank"\footnote{\url{https://ieeexplore.ieee.org/abstract/document/7098875}}. For any matrix $B$, we compute its singular value decomposition, $A = U \Lambda V.$ Then, entries along the diagonal of diagonal matrix $\Lambda$ are our singular values, $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_k.$ If we let $\sigma = (\sigma_1, \dots, \sigma_k)$, then the effective rank is equal to $\exp \left(  \sum_{i=1}^k - \frac{\sigma_i}{||\sigma||_1}  \log \left( \frac{\sigma_i}{||\sigma||_1} \right) \right)$.  \textcolor{red}{can delete or put in background lmao}

This is similar to the measure of cross-entropy! Essentially, we are measuring the entropy of the singular values. If a few singular values are dominant, then the matrix is more low-rank, as the transformation is less rich.

When computing effective rank across layers, heads, and types of weight matrix, we again saw a correlation between only the effective ranks of corresponding key and query matrices. \textcolor{red}{add figures as well} This is again summarized in our correlation matrices.


<figure>
        <p style="text-align: center;">
            $$
            \begin{array}{c|ccc}
                & \text{Q} & \text{K} & \text{V} \\
                \hline
                \text{Q} & 1 & 0.950 & 0.535 \\
                \text{K} & 0.950 & 1 & 0.542 \\
                \text{V} & 0.535 & 0.542 & 1
            \end{array}
            \qquad
            \begin{array}{c|ccc}
                & \text{Q} & \text{K} & \text{V} \\
                \hline
                \text{Q} & 1 & 0.921 & 0.348 \\
                \text{K} & 0.921 & 1 & 0.301 \\
                \text{V} & 0.348 & 0.301 & 1
            \end{array}
            $$
        </p>
        <figcaption style="text-align: center;">
            Correlation matrices for BERT large (left) and BERT base (right) for effective rank.
        </figcaption>
    </figure>


<h2>Frobenius Norm</h2>

The Frobenius norm is quite similar to the Euclidean norm for vectors—for a matrix \(A\), its defined as $\sqrt{\sum (A_{ij})^2}.$ Similar to the Euclidean norm for vectors, it can be thought of as a measurement for the size of a matrix.

Again, we were able to find a correlation between the norms of the query and key matrices, but not involving the value matrix.  

<figure>
        <p style="text-align: center;">
            $$
            \begin{array}{c|ccc}
                & \text{Q} & \text{K} & \text{V} \\
                \hline
                \text{Q} & 1 & 0.980 & -0.060 \\
                \text{K} & 0.980 & 1 & 0.036 \\
                \text{V} & -0.060 & 0.036 & 1
            \end{array}
            \qquad
            \begin{array}{c|ccc}
                & \text{Q} & \text{K} & \text{V} \\
                \hline
                \text{Q} & 1 & 0.987 & -0.266 \\
                \text{K} & 0.987 & 1 & -0.243 \\
                \text{V} & -0.266 & -0.243 & 1
            \end{array}
            $$
        </p>
        <figcaption style="text-align: center;">
            Correlation matrices for BERT large (left) and BERT base (right) for Frobenius norm.
        </figcaption>
    </figure>
    
<h2>Condition number</h2>
\begin{subsection}{Condition number}

Our last metric was the condition number, which is the ratio of the magnitude of the largest singular value to the smallest singular value. When this is large, that indicates that a certain direction dominates in the mapping performed by the matrix, which indicates that the matrix is less stable, because a small perturbation in the input is more likely to result in a substantially different output.

Again, we observe the same relationship between the query, key, and weight matrices, although the correlation for the condition number is weaker than the correlations of the other three metrics. 

<figure>
        <p style="text-align: center;">
            $$
            \begin{array}{c|ccc}
                & \text{Q} & \text{K} & \text{V} \\
                \hline
                \text{Q} & 1 & 0.912 & 0.391 \\
                \text{K} & 0.912 & 1 & 0.445 \\
                \text{V} & 0.391 & 0.445 & 1
            \end{array}
            \qquad
            \begin{array}{c|ccc}
                & \text{Q} & \text{K} & \text{V} \\
                \hline
                \text{Q} & 1 & 0.854 & 0.236 \\
                \text{K} & 0.854 & 1 & 0.269 \\
                \text{V} & 0.236 & 0.269 & 1
            \end{array}
            $$
        </p>
        <figcaption style="text-align: center;">
            Correlation matrices for BERT large (left) and BERT base (right) for condition number.
        </figcaption>
    </figure>

	<h1>Experiments on ViTs</h1>
    
		</div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
							<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>
